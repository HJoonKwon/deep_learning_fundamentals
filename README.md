# Deep Learning Fundamentals

I created this repository to deepen my understanding of fundamental models in deep learning. Specifically, I am focusing on computer vision models, including transformers and CNNs, as I use these extensively in my work. Drawing inspiration from [Andrej Karpathy](https://twitter.com/karpathy/status/1290826075916779520?lang=en) and [AI-Summer](https://theaisummer.com/einsum-attention/), I strive to write better code by utilizing tools like [einops](https://github.com/arogozhnikov/einops) and [einsum](https://pytorch.org/docs/stable/generated/torch.einsum.html). This is a long-term project for me, aimed at improving my implementation skills and growing as a machine learning engineer. While I'm not sure who else may benefit from this repository, I am committed to consistently improving and updating it.

## Sequential Models
- [x] Transformer Encoder (Attention is all you need) 
- [x] Transformer Decoder (Attention is all you need)
- [x] ViT
- [x] Swin Transformer v1
- [ ] Swin Transformer v2
- [ ] BEiT

## CNN Models 
- [ ] yolov3 
- [ ] UPerNet 

## Diffusion Models 
- [ ] TBD
